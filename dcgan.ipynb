{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "IMAGE_CHANNELS = 3\n",
    "BATCH_SIZE = 64\n",
    "DATASET_DIR = 'wikiart'\n",
    "#CLASSES = [\"Impressionism\", \"Baroque\", \"Expressionism\", \"Art_Nouveau_Modern\", \"Romanticism\"]\n",
    "CLASSES = [\"Baroque\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    image /= 127.5\n",
    "    image -= 1\n",
    "    return image\n",
    "\n",
    "def get_dataset():\n",
    "    img_gen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "    generator_method = img_gen.flow_from_directory(\n",
    "        DATASET_DIR, \n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "        classes = CLASSES, \n",
    "        batch_size = 1)\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: generator_method,\n",
    "        output_types=(tf.float32, tf.float32), \n",
    "        output_shapes=([1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS], [1, len(CLASSES)])\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4240 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 500\n",
    "STEPS_PER_EPOCH = 4240  // BATCH_SIZE\n",
    "train_dataset = get_dataset().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIMS = 100\n",
    "LEAKY_RELU_ALPHA = 0.2\n",
    "\n",
    "def create_generator(): \n",
    "    weight_initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    \n",
    "    def conv2d_transponse_layer(input, filters, kernel=5):\n",
    "        conv = layers.Conv2DTranspose(filters, (kernel, kernel), strides=(2, 2), padding='same', use_bias=False, \n",
    "                                  kernel_initializer=weight_initializer)(input)\n",
    "        batch = layers.BatchNormalization()(conv)\n",
    "        return layers.LeakyReLU(LEAKY_RELU_ALPHA)(batch)\n",
    "    \n",
    "    noise = layers.Input(shape=(NOISE_DIMS,))\n",
    "    label = layers.Input(shape=(len(CLASSES),))\n",
    "    input = layers.Concatenate()([noise, label])\n",
    "    \n",
    "    dense = layers.Dense(4*4*1024, use_bias=False, kernel_initializer=weight_initializer)(input)    \n",
    "    reshaped = layers.Reshape((4, 4, 1024))(dense)\n",
    "    \n",
    "    conv1 = conv2d_transponse_layer(reshaped, filters=512)\n",
    "    conv2 = conv2d_transponse_layer(conv1, filters=256)\n",
    "    conv3 = conv2d_transponse_layer(conv2, filters=128)\n",
    "    #conv4 = conv2d_transponse_layer(conv3, filters=256)\n",
    "    \n",
    "    out = layers.Conv2DTranspose(IMAGE_CHANNELS, (3, 3), strides=(2, 2), padding='same', use_bias=False, \n",
    "                                 activation='tanh')(conv3)\n",
    "    print(out.shape)\n",
    "    return tf.keras.Model(inputs=[noise, label], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(): \n",
    "    weight_initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    \n",
    "    def conv2d_layer(input, filters, stride=2):\n",
    "        conv = layers.Conv2D(filters, (5, 5), strides=(stride, stride), padding='same',\n",
    "                             kernel_initializer= weight_initializer)(input)\n",
    "        batch = layers.BatchNormalization()(conv)\n",
    "        relu = layers.LeakyReLU(LEAKY_RELU_ALPHA)(batch)\n",
    "        dropout = layers.Dropout(0.25)\n",
    "        return relu\n",
    "    \n",
    "    image = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
    "    label = layers.Input(shape=(len(CLASSES),))\n",
    "    \n",
    "    #random noise\n",
    "    noise = layers.GaussianNoise(0.2)(image)\n",
    "    \n",
    "    conv = layers.Conv2D(16, (5, 5), strides=(2, 2), padding='same',\n",
    "                             kernel_initializer= weight_initializer)(noise)\n",
    "    relu = layers.LeakyReLU(LEAKY_RELU_ALPHA)(conv)\n",
    "    drop = layers.Dropout(0.25)(relu)\n",
    "    \n",
    "    conv2 = conv2d_layer(drop, filters=32)\n",
    "    conv3 = conv2d_layer(conv2, filters=64)\n",
    "    conv4 = conv2d_layer(conv3, filters=128)\n",
    "    print(conv4.shape)\n",
    "    \n",
    "    img_out = layers.Flatten()(conv)\n",
    "    merged = layers.Concatenate()([img_out, label])\n",
    "    out = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    return tf.keras.Model(inputs=[image, label], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n",
      "(None, 4, 4, 128)\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    #label smoothing\n",
    "    real_loss = cross_entropy((tf.random.uniform(real_output.shape.as_list(), minval=0.7, maxval=1)), real_output)\n",
    "    fake_loss = cross_entropy(tf.random.uniform(fake_output.shape.as_list(), minval=0.0, maxval=0.3), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    #label smoothing\n",
    "    return cross_entropy(tf.random.uniform(fake_output.shape, minval=0.7, maxval=1.2), fake_output)\n",
    "    #return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=0.5)\n",
    "\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels, step):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIMS])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise, labels], training=True)\n",
    "        \n",
    "        real_output = discriminator([images, labels], training=True)    \n",
    "        fake_output = discriminator([generated_images, labels], training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        \n",
    "        #gen_loss_metric(gen_loss)\n",
    "        #disc_loss_metric(disc_loss)\n",
    "        \n",
    "        discriminator.trainable = False\n",
    "        fake_output2 = discriminator([generated_images, labels], training=False)\n",
    "        gen_loss = generator_loss(fake_output2) \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        \n",
    "        if(step % 20 == 0):\n",
    "            print('Step {}, Gen loss {}, Disc loss {}, Dx {}, DGz1 {}, DGz2 {} '.format(step, gen_loss.numpy(), \n",
    "                                                                               disc_loss.numpy(),\n",
    "                                                                               np.mean(real_output.numpy()), \n",
    "                                                                               np.mean(fake_output.numpy()),\n",
    "                                                                               np.mean(fake_output2.numpy())))\n",
    "        \n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([1, NOISE_DIMS])\n",
    "label = tf.constant([[1]])\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    print(\"Starting epoch...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    step = 1\n",
    "    for image_batch, label_batch in dataset:\n",
    "      print(\"Step \", step)\n",
    "      train_step(tf.squeeze(image_batch), tf.squeeze(label_batch), step)\n",
    "      if(step == STEPS_PER_EPOCH): \n",
    "        break\n",
    "      step = step + 1\n",
    "\n",
    "    \n",
    "    res = generator([seed, label], training=False).numpy()\n",
    "\n",
    "    for pic in res:\n",
    "        pic = pic * 127.5 + 127.5\n",
    "        img = Image.fromarray(pic.astype('uint8'))\n",
    "        img.save(\"pic{}.png\".format(epoch))\n",
    "    \n",
    "    #with train_summary_writer.as_default():\n",
    "     #   tf.summary.scalar('Gen loss', gen_loss_metric.result(), step=epoch)\n",
    "     #   tf.summary.scalar('Disc loss', disc_loss_metric.result(), step=epoch)\n",
    "        \n",
    "    #template = 'Epoch {}, Gen loss: {}, Disc loss: {}'\n",
    "    #print(template.format(epoch+1,gen_loss_metric.result(), disc_loss_metric.result()))\n",
    "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch...\n",
      "Step  1\n",
      "Step  2\n",
      "Step  3\n",
      "Step  4\n",
      "Step  5\n",
      "Step  6\n",
      "Step  7\n",
      "Step  8\n",
      "Step  9\n",
      "Step  10\n",
      "Step  11\n",
      "Step  12\n",
      "Step  13\n",
      "Step  14\n",
      "Step  15\n",
      "Step  16\n",
      "Step  17\n",
      "Step  18\n",
      "Step  19\n",
      "Step  20\n",
      "Step 20, Gen loss 0.3966459035873413, Disc loss 1.618373155593872, Dx 0.712865948677063, DGz1 0.9490775465965271, DGz2 0.9467740058898926 \n",
      "Step  21\n",
      "Step  22\n",
      "Step  23\n",
      "Step  24\n",
      "Step  25\n",
      "Step  26\n",
      "Step  27\n",
      "Step  28\n",
      "Step  29\n",
      "Step  30\n",
      "Step  31\n",
      "Step  32\n",
      "Step  33\n",
      "Step  34\n",
      "Step  35\n",
      "Step  36\n",
      "Step  37\n",
      "Step  38\n",
      "Step  39\n",
      "Step  40\n",
      "Step 40, Gen loss 0.3474189341068268, Disc loss 1.6335217952728271, Dx 0.8344216346740723, DGz1 0.9848053455352783, DGz2 0.9848477244377136 \n",
      "Step  41\n",
      "Step  42\n",
      "Step  43\n",
      "Step  44\n",
      "Step  45\n",
      "Step  46\n",
      "Step  47\n",
      "Step  48\n",
      "Step  49\n",
      "Step  50\n",
      "Step  51\n",
      "Step  52\n",
      "Step  53\n",
      "Step  54\n",
      "Step  55\n",
      "Step  56\n",
      "Step  57\n",
      "Step  58\n",
      "Step  59\n",
      "Step  60\n",
      "Step 60, Gen loss 0.33042484521865845, Disc loss 1.6421608924865723, Dx 0.8953465223312378, DGz1 0.9944698810577393, DGz2 0.9946238994598389 \n",
      "Step  61\n",
      "Step  62\n",
      "Step  63\n",
      "Step  64\n",
      "Step  65\n",
      "Step  66\n",
      "Time for epoch 1 is 551.1914489269257 sec\n",
      "Starting epoch...\n",
      "Step  1\n",
      "Step  2\n",
      "Step  3\n",
      "Step  4\n",
      "Step  5\n",
      "Step  6\n",
      "Step  7\n",
      "Step  8\n",
      "Step  9\n",
      "Step  10\n",
      "Step  11\n",
      "Step  12\n",
      "Step  13\n",
      "Step  14\n",
      "Step  15\n",
      "Step  16\n",
      "Step  17\n",
      "Step  18\n",
      "Step  19\n",
      "Step  20\n",
      "Step 20, Gen loss 0.371315598487854, Disc loss 1.626610279083252, Dx 0.9353894591331482, DGz1 0.9990347623825073, DGz2 0.999079704284668 \n",
      "Step  21\n",
      "Step  22\n",
      "Step  23\n",
      "Step  24\n",
      "Step  25\n",
      "Step  26\n",
      "Step  27\n",
      "Step  28\n",
      "Step  29\n",
      "Step  30\n",
      "Step  31\n",
      "Step  32\n",
      "Step  33\n",
      "Step  34\n",
      "Step  35\n",
      "Step  36\n",
      "Step  37\n",
      "Step  38\n",
      "Step  39\n",
      "Step  40\n",
      "Step 40, Gen loss 0.38748377561569214, Disc loss 1.64720618724823, Dx 0.9564461708068848, DGz1 0.9997338056564331, DGz2 0.9997776746749878 \n",
      "Step  41\n",
      "Step  42\n",
      "Step  43\n",
      "Step  44\n",
      "Step  45\n",
      "Step  46\n",
      "Step  47\n",
      "Step  48\n",
      "Step  49\n",
      "Step  50\n",
      "Step  51\n",
      "Step  52\n",
      "Step  53\n",
      "Step  54\n",
      "Step  55\n",
      "Step  56\n",
      "Step  57\n",
      "Step  58\n",
      "Step  59\n",
      "Step  60\n",
      "Step 60, Gen loss 0.3660341501235962, Disc loss 1.608130931854248, Dx 0.9590929746627808, DGz1 0.9998874664306641, DGz2 0.9998947978019714 \n",
      "Step  61\n",
      "Step  62\n",
      "Step  63\n",
      "Step  64\n",
      "Step  65\n",
      "Step  66\n",
      "Time for epoch 2 is 541.6790430545807 sec\n",
      "Starting epoch...\n",
      "Step  1\n",
      "Step  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2acff166190e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-f9df3a19b881>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m       \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m       \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-b3af87883072>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(images, labels, step)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNOISE_DIMS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mgenerated_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mreal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    715\u001b[0m                                 ' implement a `call` method.')\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m     return self._run_internal_graph(\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[0moutput_shape_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m     outputs = backend.conv2d_transpose(\n\u001b[0m\u001b[0;32m    973\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(x, kernel, output_shape, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   5035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5036\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdilation_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5037\u001b[1;33m     x = nn.conv2d_transpose(x, kernel, output_shape, strides,\n\u001b[0m\u001b[0;32m   5038\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5039\u001b[0m                             data_format=tf_data_format)\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(value, filter, output_shape, strides, padding, data_format, name, input, filters, dilations)\u001b[0m\n\u001b[0;32m   2204\u001b[0m   with ops.name_scope(name, \"conv2d_transpose\",\n\u001b[0;32m   2205\u001b[0m                       [value, filter, output_shape]) as name:\n\u001b[1;32m-> 2206\u001b[1;33m     return conv2d_transpose_v2(\n\u001b[0m\u001b[0;32m   2207\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose_v2\u001b[1;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2281\u001b[0m     \u001b[0mdilations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m     return gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[0;32m   2284\u001b[0m         \u001b[0minput_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lruzhinski\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1234\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   1237\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Conv2DBackpropInput\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generator([tf.random.normal([1, NOISE_DIMS]), tf.constant([[1]])], training=False).numpy()\n",
    "\n",
    "for pic in res:\n",
    "    pic = pic * 127.5 + 127.5\n",
    "    img = Image.fromarray(pic.astype('uint8'))\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
